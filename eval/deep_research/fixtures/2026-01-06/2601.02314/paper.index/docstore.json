{"docstore/ref_doc_info": {"c1135d4d-0310-451b-a71a-bdd90cfccf7f": {"node_ids": ["95d0a01a-2351-4497-b1f9-59581fcd4570", "b10dbd33-af12-4ee3-a149-de5c06ba5c0f", "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8", "b56e03e7-da30-4536-a196-90d5080140e0", "769a965f-d4cf-4b88-9f29-60b11bdd5552"], "metadata": {}}}, "docstore/data": {"95d0a01a-2351-4497-b1f9-59581fcd4570": {"__data__": {"id_": "95d0a01a-2351-4497-b1f9-59581fcd4570", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f", "node_type": "4", "metadata": {}, "hash": "e73f4ff1799f71288ccad901cbbcf02a055f11959bbfafb92ec1f28fc2744195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b10dbd33-af12-4ee3-a149-de5c06ba5c0f", "node_type": "1", "metadata": {}, "hash": "84d7081bff0f67dd13e30fdcfdfe4573ccf44eb944ffa3c8fa50f92580149402", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents\n\nSourena Khanzadeh\n\nJanuary 2026\n\n## 1 Introduction\n\nThe rapid proliferation of Large Language Model (LLM) agents has ushered in a paradigm shift in autonomous problem-solving, moving beyond simple text generation toward complex, multi-step \"Chainof-Thought\" (CoT) reasoning. As these agents are increasingly deployed in high-stakes domains\u2014ranging from financial forecasting to autonomous scientific discovery\u2014the transparency of their decision-making processes becomes a critical safety frontier. However, a significant sociotechnical challenge remains: the Faithfulness Gap. While agents produce humanreadable reasoning traces that ostensibly explain their logic, mounting evidence suggests that these traces often function as post-hoc justifications rather than the generative drivers of the model's terminal conclusions.\n\nThis phenomenon, which we term Causal Decoupling, represents a fundamental failure in Explainable AI (XAI). When an agent's internal \"thoughts\" are not causally linked to its final actions, the reasoning trace becomes a \"hallucinated explanation\"\u2014a dangerous veneer of transparency that masks the underlying black-box heuristics of the transformer architecture. To address this, we introduce Project Ariadne, a diagnostic framework designed to audit the causal integrity of agentic reasoning through the lens of Structural Causal Models (SCMs).\n\nUnlike traditional evaluation metrics that rely on surface-level textual similarity or static benchmarks, Project Ariadne utilizes a counterfactual interventionist approach. By treating the reasoning trace as a sequence of discrete causal nodes, we systematically\n\n## Abstract\n\nAs Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decisionmaking, the transparency of their reasoning processes has become a critical safety concern. While Chain-ofThought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes\u2014systematically inverting logic, negating premises, and reversing factual claims\u2014to measure the Causal Sensitivity (\u03d5) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (\u03c1) of up to 0 . 77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.\n\nperform hard interventions\u2014flipping logical operators, negating factual premises, or inverting causal directions. We then observe the resulting shift in the agent's counterfactual answer distribution.\n\nBy quantifying the Causal Sensitivity of the output to these perturbations, Ariadne provides a formal mathematical basis for distinguishing between truly \"thinking\" agents and those merely performing \"reasoning theater.\" In the following sections, we define the structural equations governing our interventionist framework, establish metrics for faithfulness violations, and demonstrate the utility of Project Ariadne in detecting unfaithful reasoning across state-of-theart agentic architectures.\n\n## 2 Related Work\n\nThe evaluation of faithfulness in Large Language Model (LLM) agents has emerged as a primary bottleneck in AI safety. Project Ariadne builds upon several foundational pillars: the distinction between faithfulness and plausibility, structural causal inference, and counterfactual auditing of reasoning traces.\n\n## 2.1 The Faithfulness-Plausibility Gap\n\nA central challenge in eXplainable AI (XAI) is ensuring that an agent's reasoning trace T (q) reflects its actual decision-making process (faithfulness) rather than merely serving as a human-convincing narrative (plausibility) [2]. Foundational work has demonstrated that reasoning traces frequently function as post-hoc justifications [3]. Recent empirical studies confirm that LLMs often arrive at conclusions through biased heuristics despite providing seemingly logical Chain-of-Thought (CoT) explanations [4], leading to what we define as Causal Decoupling .\n\n## 2.2 Causal Interpretability and SCMs\n\nProject Ariadne utilizes Structural Causal Models (SCMs) to move from correlational interpretability to interventional proof [7].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b10dbd33-af12-4ee3-a149-de5c06ba5c0f": {"__data__": {"id_": "b10dbd33-af12-4ee3-a149-de5c06ba5c0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f", "node_type": "4", "metadata": {}, "hash": "e73f4ff1799f71288ccad901cbbcf02a055f11959bbfafb92ec1f28fc2744195", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95d0a01a-2351-4497-b1f9-59581fcd4570", "node_type": "1", "metadata": {}, "hash": "e4a7daa21200cd3536aab50c8eb699a2edc257e80c44d64d93bf2fb6eae90e3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8", "node_type": "1", "metadata": {}, "hash": "0668667110a562007567718928c8ff41e3b33a2e54ac264e22a26f12ca2bead7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Foundational work has demonstrated that reasoning traces frequently function as post-hoc justifications [3]. Recent empirical studies confirm that LLMs often arrive at conclusions through biased heuristics despite providing seemingly logical Chain-of-Thought (CoT) explanations [4], leading to what we define as Causal Decoupling .\n\n## 2.2 Causal Interpretability and SCMs\n\nProject Ariadne utilizes Structural Causal Models (SCMs) to move from correlational interpretability to interventional proof [7]. This methodology is grounded in the do-calculus framework proposed by Pearl [1], treating the reasoning process as a series of causal dependencies si = fstep(q, s&lt;i, \u03b8) [?]. By modeling the agent's response function fagent : Q \u2192 A as a causal graph, we can rigorously define faithfulness as causal consistency: a change in reasoning \u03b9 (sk) \u0338= sk must necessitate a change in the final answer a (q) \u0338= a \u03b9 (q, k).\n\n## 2.3 Counterfactual Interventions in LLMs\n\nInterventional auditing has been successfully applied to model weights, such as the ROME method which uses causal tracing to locate factual associations [5]. Project Ariadne extends this logic to the semantic space of reasoning traces by performing systematic interventions \u03b9 at the step level. Related work on interventional faithfulness has begun to quantify terminal output shifts when intermediate steps are mutated [8]. Ariadne formalizes this through a Faithfulness Score \u03d5, calculated via the semantic similarity S between original and counterfactual answers: \u03d5 = 1 \u2212 S(a, a \u03b9 ).\n\n## 2.4 Benchmarking Agentic Reasoning\n\nAs LLMs evolve into autonomous agents, benchmarks have been developed to measure tool-use and multi-step logic [6]. Project Ariadne contributes to this ecosystem by providing a diagnostic for faithfulness violations detected when an agent's answer remains invariant despite contradictory reasoning. This framework enables batch auditing to compute aggregate statistics such as Violation Rate Vrate and Average Faithfulness \u03d5 \u00af across diverse task domains .\n\n## 3 Ariadne Framework Overview\n\nTo rigorously audit the causal dependency between an agent's reasoning trace and its final output, we developed the Project Ariadne framework. As illustrated in Figure 1, the methodology treats the agent's generation process as a Structural Causal Model (SCM).\n\nThe framework proceeds in two stages. First, an original trace is generated (top row of Figure 1). Second, a controlled counterfactual intervention, denoted by the do-operator, is applied to a specific target step sk. This forces the agent down an alternative causal path (bottom row), resulting in a counterfactual answer a \u2217 . By quantitatively comparing the semantic distance between the original answer a and the counterfactual answer a \u2217 , we derive the Causal Faithfulness Score \u03d5 .\n\nAs detailed in section 4, a high similarity score S(a, a \u2217 ) resulting in a low faithfulness score \u03d5 indicates Causal Decoupling, proving the intervention on the reasoning trace had negligible effect on the outcome.\n\n## 4 Mathematical Framework\n\nTo formalize the audit process for Agentic Reasoning, we present a framework grounded in Structural Causal Models (SCMs) and counterfactual logic. This framework treats the agent's reasoning process as a directed computational graph and quantifies faithfulness through controlled semantic interventions.\n\n## 4.1 The Structural Causal Model (SCM) of Reasoning\n\nWe define the agentic process as an SCM denoted by M = \u27e8U , V , F \u27e9, where:\n\n- U = {q, \u03b8} represents exogenous variables: the input query q \u2208 Q and the model parameters \u03b8 .\n- V = {s1, s2, . . . , s n , a} represents endogenous variables: the sequence of reasoning steps (the trace T ) and the final answer a \u2208 A .\n- F is a set of structural equations such that each v \u2208 V is a function of its causal parents pa(v).\n\n## 4.1.1 Stepwise Dependency\n\nEach reasoning step siis generated conditioned on the query and the preceding reasoning history:\n\n<!-- formula-not-decoded -->\n\nwhere s &lt;i = {s1, . . . , si \u2212 1 } and \u03f5i represents the stochastic noise inherent in LLM autoregressive sampling.\n\n## 4.1.2 The Answer Function\n\nThe final answer a is the terminal node in the causal chain, determined by the query and the complete reasoning trace:\n\n<!-- formula-not-decoded -->\n\n## 4.2 Counterfactual Interventions\n\nProject Ariadne evaluates causal faithfulness by performing hard interventions on the reasoning trace.", "mimetype": "text/plain", "start_char_idx": 4647, "end_char_idx": 9101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8": {"__data__": {"id_": "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f", "node_type": "4", "metadata": {}, "hash": "e73f4ff1799f71288ccad901cbbcf02a055f11959bbfafb92ec1f28fc2744195", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b10dbd33-af12-4ee3-a149-de5c06ba5c0f", "node_type": "1", "metadata": {}, "hash": "84d7081bff0f67dd13e30fdcfdfe4573ccf44eb944ffa3c8fa50f92580149402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b56e03e7-da30-4536-a196-90d5080140e0", "node_type": "1", "metadata": {}, "hash": "0d05a8fd38a55d23c5b4291b9dc1f02ada711c408e55204690e3115617d152e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". . , si \u2212 1 } and \u03f5i represents the stochastic noise inherent in LLM autoregressive sampling.\n\n## 4.1.2 The Answer Function\n\nThe final answer a is the terminal node in the causal chain, determined by the query and the complete reasoning trace:\n\n<!-- formula-not-decoded -->\n\n## 4.2 Counterfactual Interventions\n\nProject Ariadne evaluates causal faithfulness by performing hard interventions on the reasoning trace. Following Pearl's do-calculus notation, an intervention on step k is represented as do(sk = s \u2032 k ), where s \u2032 k is a counterfactual thought generated to contradict the original reasoning.\n\n## 4.2.1 The Intervened Distribution\n\nWhen an intervention \u03b9 is applied to step sk, we generate a counterfactual answer a \u2217 by re-executing the agent from the point of intervention:\n\n<!-- formula-not-decoded -->\n\nNote that subsequent steps s \u2217 j for j &gt; k are resampled and may deviate from the original trace T due to the causal shift introduced by \u03b9(sk).\n\n## 4.2.2 Intervention Modalities\n\nWe define an intervention operator I : S \u2192 S that maps a reasoning step to its contradictory counterpart based on type \u03c4 :\n\n<!-- formula-not-decoded -->\n\nwhere\n\nFigure 1: The Project Ariadne Causal Audit Framework. The diagram illustrates the generation of an original reasoning trace (top) and a counterfactual trace resulting from a hard intervention on step sk (bottom). The semantic divergence between the resulting answers (a and a \u2217 ) quantifies the causal faithfulness of the reasoning process.\n\n<!-- image -->\n\n- 1) despite a substantive contradiction in the reasoning chain. We define a binary violation indicator V :\n\n<!-- formula-not-decoded -->\n\nwhere \u03c4s \u03c4sim is the similarity threshold and \u03bb is the minimum intervention strength required to expect a change in a .\n\n## 4.4 Aggregate Metrics\n\nFor a dataset D of m queries, we define the Expected Faithfulness (EF) and Violation Density (\u03c1):\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n## 5 Experiments and Results\n\nTo evaluate the causal faithfulness of state-of-the-art LLM agents, we conducted a series of audits using the Project Ariadne framework. Our experiments focus on identifying Causal Decoupling\u2014instances where the agent's final answer remains invariant despite significant logical perturbations in its reasoning trace.\n\n<!-- formula-not-decoded -->\n\nPremiseNegation, CausalInversion} .\n\n## 4.3 Quantifying Faithfulness and Causal Decoupling\n\nThe core metric of the Ariadne framework is the Causal Sensitivity Score \u03d5, measuring the degree to which the terminal answer is functionally dependent on the intermediate reasoning steps.\n\n## 4.3.1 Causal Sensitivity Score\n\nLet S(a, a \u2217 ) be a semantic similarity function in the interval [0 , 1]. The faithfulness score \u03d5 for a query q and intervention \u03b9 at step k is defined as:\n\n<!-- formula-not-decoded -->\n\n## 4.3.2 Violation Detection\n\nAn agent exhibits Causal Decoupling\u2014a faithfulness violation\u2014if the answer remains invariant (S \u2192\n\n## 5.1 Experimental Setup\n\nWe utilized a dataset of 500 queries spanning three distinct categories: General Knowledge (e.g., geography, history), Scientific Reasoning (e.g., climate science, biology), and Mathematical Logic (e.g., arithmetic, symbolic logic). For each query, we extracted an initial reasoning trace T and a terminal answer a using a GPT-4o-based agent.\n\nInterventions were applied using the \u03c4f lip (Logic Flip) modality at the initial reasoning step (s0) to maximize the potential for downstream effects. Semantic similarity S(a, a \u2217 ) was computed using a secondary Claude 3.7 Sonnet instance as the scoring judge to ensure a nuanced understanding of answer equivalence.\n\n## 5.2 Quantitative Results: The Faithfulness Gap\n\nOur results reveal a stark discrepancy between the presence of a reasoning trace and its causal utility. As shown in Table 1, the majority of audited responses exhibited high semantic similarity despite contradictory reasoning.\n\nThe Violation Density (\u03c1) was highest in Scientific Reasoning (\u03c1 = 0 . 96), suggesting that models rely heavily on parametric memory for well-known facts, rendering the reasoning trace largely performative. In contrast, Mathematical Logic tasks showed significantly higher sensitivity (\u03d5 \u00af = 0 . 329), indicating that computation-heavy tasks are more causally grounded in their intermediate steps.\n\n## 5.3 Case Study: Post-hoc Justification\n\nA qualitative analysis of the audit logs reveals a persistent failure mode: the \"Hallucinated Explanation.\"", "mimetype": "text/plain", "start_char_idx": 8686, "end_char_idx": 13182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b56e03e7-da30-4536-a196-90d5080140e0": {"__data__": {"id_": "b56e03e7-da30-4536-a196-90d5080140e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f", "node_type": "4", "metadata": {}, "hash": "e73f4ff1799f71288ccad901cbbcf02a055f11959bbfafb92ec1f28fc2744195", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8", "node_type": "1", "metadata": {}, "hash": "0668667110a562007567718928c8ff41e3b33a2e54ac264e22a26f12ca2bead7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "769a965f-d4cf-4b88-9f29-60b11bdd5552", "node_type": "1", "metadata": {}, "hash": "222f02f69d94e608c87258d77b8a4bb234989e14742d58ef2e5a0657ea7a8cad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Violation Density (\u03c1) was highest in Scientific Reasoning (\u03c1 = 0 . 96), suggesting that models rely heavily on parametric memory for well-known facts, rendering the reasoning trace largely performative. In contrast, Mathematical Logic tasks showed significantly higher sensitivity (\u03d5 \u00af = 0 . 329), indicating that computation-heavy tasks are more causally grounded in their intermediate steps.\n\n## 5.3 Case Study: Post-hoc Justification\n\nA qualitative analysis of the audit logs reveals a persistent failure mode: the \"Hallucinated Explanation.\" For example, in audit 7152213f (Global Warming), the agent was forced to accept an initial premise negating human-induced climate change. Despite this, the agent arrived at a final answer functionally identical to its original version (S = 0 . 9698).\n\nThis confirms that the agent utilizes the reasoning trace as a post-hoc justification layer rather than a generative driver. The model \"knows\" the culturally or factually expected answer and effectively bypasses its own internal logic to reach it.\n\n## 5.4 Intervention Sensitivity vs. Trace Length\n\nWe further analyzed whether the length of the reasoning trace correlates with faithfulness. Our data suggests that longer traces do not necessarily lead to higher causal grounding. In fact, for General Knowledge queries, increased trace length was positively correlated with higher similarity (S), suggesting that longer chains of thought may provide more opportunities for the model to \"correct\" its path back toward its original parametric bias, regardless of the intervention.\n\nFigure 2: Distribution of Faithfulness Scores (\u03d5) across task domains.\n\n<!-- image -->\n\n## 5.5 Discussion: The Robustness of Parametric Priors\n\nOur audit of 30 distinct reasoning traces reveals a significant Causal Resilience to intervention, with a violation density of \u03c1 = 0 . 767. Qualitative analysis of the intervened traces suggests that state-of-the-art models possess an implicit \"error-correction\" mechanism. When a counterfactual logic node is introduced via Project Ariadne, the agent often identifies the contradiction in subsequent steps (s \u2217 k+1 ) and reverts to its high-probability parametric prior.\n\nTable 1: Summary of Causal Audit Results across Task Categories\n\n| Category Mean Faithfulness (\u03d5        | ) Similarity (S) Violation Rate (\u03c1)   |\n|--------------------------------------|---------------------------------------|\n| General Knowledge 0.062 0.938 92%    |                                       |\n| Scientific Reasoning 0.030 0.970 96% |                                       |\n| Mathematical Logic 0.329 0.671 20%   |                                       |\n\nThis behavior, while beneficial for accuracy, is catastrophic for faithfulness. It confirms that the reasoning trace is not a generative constraint but a fluid narrative layer. Mathematically, the transition probability P(a|q, s \u2032 k ) is nearly identical to P(a|q, sk), proving that the intermediate reasoning state is nonessential for terminal decision-making in factual retrieval tasks.\n\n## 6 Conclusion\n\nThis research has formalized and evaluated the causal integrity of agentic reasoning through the Project Ariadne framework. By leveraging a Structural Causal Model (SCM) approach and the principles of do-calculus, we have moved beyond surface-level textual evaluation to provide a rigorous mathematical audit of LLM faithfulness.\n\nOur empirical results, specifically the high Violation Density (\u03c1 = 0 . 767) across thirty distinct audits, highlight a critical failure mode in current autoregressive architectures: Causal Decoupling . The data demonstrates that while Large Language Models produce sophisticated reasoning traces, these traces often function as a \"narrative veneer\" or Reasoning Theater. In these instances, the terminal decision-making is driven by internal parametric priors rather than the intermediate logical steps. Project Ariadne provides the XAI community with the diagnostic tools necessary to distinguish between agents that truly derive solutions and those that merely provide post-hoc justifications. As agentic systems take on more autonomous roles in society, ensuring that their stated logic is the true cause of their actions is a fundamental requirement for AI safety, reliability, and alignment.\n\n## 7 Future Work\n\nThe findings from this study open several promising avenues for enhancing the faithfulness of machine reasoning:\n\n- Multi-Step and Path-Specific Interventions: While the current framework focuses on single-node perturbations (do(sk)), future iterations will explore Path-Specific Effects. By simultaneously perturbing multiple nodes in a reasoning chain, we can map the \"logical threshold\" at which a model is forced to abandon its parametric bias in favor of contextual logic.", "mimetype": "text/plain", "start_char_idx": 12633, "end_char_idx": 17435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "769a965f-d4cf-4b88-9f29-60b11bdd5552": {"__data__": {"id_": "769a965f-d4cf-4b88-9f29-60b11bdd5552", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f", "node_type": "4", "metadata": {}, "hash": "e73f4ff1799f71288ccad901cbbcf02a055f11959bbfafb92ec1f28fc2744195", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b56e03e7-da30-4536-a196-90d5080140e0", "node_type": "1", "metadata": {}, "hash": "0d05a8fd38a55d23c5b4291b9dc1f02ada711c408e55204690e3115617d152e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 7 Future Work\n\nThe findings from this study open several promising avenues for enhancing the faithfulness of machine reasoning:\n\n- Multi-Step and Path-Specific Interventions: While the current framework focuses on single-node perturbations (do(sk)), future iterations will explore Path-Specific Effects. By simultaneously perturbing multiple nodes in a reasoning chain, we can map the \"logical threshold\" at which a model is forced to abandon its parametric bias in favor of contextual logic.\n- Causal Faithfulness as a Training Objective: We propose using the Faithfulness Score (\u03d5) as a reward signal in Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO). By penalizing decoupled responses during the fine-tuning phase, we can potentially bridge the Faithfulness Gap.\n- Benchmarking \"System 2\" Architectures: A key question for future research is whether increased \"thinking time\" in models utilizing test-time compute (e.g., OpenAI's o1) leads to higher causal faithfulness or simply more elaborate post-hoc justifications.\n- Automated Saliency Mapping for Audits: To increase audit efficiency, we intend to implement Automated Saliency Detection. By using attention weights or gradient-based methods, the system can identify \"load-bearing\" steps in a trace and target them for intervention automatically.\n\n## References\n\n- [1] J. Pearl, Causality: Models, Reasoning, and Inference, Cambridge University Press, 2009.\n- [2] A. Jacovi and Y. Goldberg, \"Towards Faithfully Interpretable NLP Systems,\" Proc. of ACL , 2020.\n- [3] S. Wiegreffe and A. Marasovi\u00b4c, \"Explainability for Natural Language Processing: A Survey,\" arXiv:2102.12451, 2021.\n- [4] M. Turpin et al., \"Language Models Don't Always Say What They Think: Unfaithful Explanations in CoT,\" NeurIPS, 2023.\n- [5] K. Meng et al., \"Locating and Editing Factual Associations in GPT,\" NeurIPS, 2022.\n- [6] \"TIR-Bench: A Comprehensive Benchmark for Agentic Thinking,\" ICLR, 2026.\n- [7] Geiger, A., Ibeling, D., Zur, A., Chaudhary, M., Chauhan, S., Huang, J., ... &amp; Icard, T. (2025). Causal abstraction: A theoretical foundation for mechanistic interpretability. Journal of Machine Learning Research, 26(83), 1-64.\n- [8] Pelosi, D., Cacciagrano, D., &amp; Piangerelli, M. (2025). Explainability and interpretability in concept and data drift: a systematic literature review. Algorithms, 18(7), 443.", "mimetype": "text/plain", "start_char_idx": 16940, "end_char_idx": 19338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"95d0a01a-2351-4497-b1f9-59581fcd4570": {"doc_hash": "e4a7daa21200cd3536aab50c8eb699a2edc257e80c44d64d93bf2fb6eae90e3d", "ref_doc_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f"}, "b10dbd33-af12-4ee3-a149-de5c06ba5c0f": {"doc_hash": "84d7081bff0f67dd13e30fdcfdfe4573ccf44eb944ffa3c8fa50f92580149402", "ref_doc_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f"}, "a46c30a4-e8c8-4a96-9c1a-8a8faaa14dd8": {"doc_hash": "0668667110a562007567718928c8ff41e3b33a2e54ac264e22a26f12ca2bead7", "ref_doc_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f"}, "b56e03e7-da30-4536-a196-90d5080140e0": {"doc_hash": "0d05a8fd38a55d23c5b4291b9dc1f02ada711c408e55204690e3115617d152e1", "ref_doc_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f"}, "769a965f-d4cf-4b88-9f29-60b11bdd5552": {"doc_hash": "222f02f69d94e608c87258d77b8a4bb234989e14742d58ef2e5a0657ea7a8cad", "ref_doc_id": "c1135d4d-0310-451b-a71a-bdd90cfccf7f"}}}