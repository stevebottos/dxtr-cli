{
  "publishedAt": "2026-01-05T18:05:29.000Z",
  "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
  "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (\u03c6) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (\u03c1) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
  "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02314.png",
  "numComments": 1,
  "submittedBy": {
    "_id": "66206e4ed0eb50531a28b4df",
    "avatarUrl": "/avatars/24a05b21e213bc8697bda4f5a6db7c51.svg",
    "fullname": "Sourena Khanzadeh",
    "name": "Suren15",
    "type": "user",
    "isPro": false,
    "isHf": false,
    "isHfAdmin": false,
    "isMod": false,
    "isUserFollowing": false
  },
  "organization": {
    "_id": "65ad969414d782df06bf4eb4",
    "name": "TorontoMetropolitanUniversity",
    "fullname": "Toronto Metropolitan University",
    "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/nlhGsTSfCkCvWwqS4MwLD.png"
  },
  "isAuthorParticipating": false,
  "id": "2601.02314",
  "authors": [
    {
      "_id": "695c84466aa73bc11f0914aa",
      "name": "Sourena Khanzadeh",
      "hidden": false
    }
  ],
  "submittedOnDailyAt": "2026-01-06T01:39:32.034Z",
  "submittedOnDailyBy": {
    "_id": "66206e4ed0eb50531a28b4df",
    "avatarUrl": "/avatars/24a05b21e213bc8697bda4f5a6db7c51.svg",
    "isPro": false,
    "fullname": "Sourena Khanzadeh",
    "user": "Suren15",
    "type": "user"
  },
  "upvotes": 0,
  "discussionId": "695c84466aa73bc11f0914ab",
  "githubRepo": "https://github.com/skhanzad/AridadneXAI",
  "githubRepoAddedBy": "user",
  "ai_summary": "Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.",
  "ai_keywords": [
    "Large Language Model agents",
    "Chain-of-Thought prompting",
    "Structural Causal Models",
    "counterfactual logic",
    "do-calculus",
    "Causal Sensitivity",
    "Faithfulness Gap",
    "Causal Decoupling",
    "Ariadne Score"
  ],
  "githubStars": 1
}