import asyncio
import json
import uvicorn
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from pydantic_ai.messages import ModelMessage

from dxtr import set_session_id, get_model_settings, run_agent, create_event_queue, clear_event_queue
from dxtr.agents.master import agent as main_agent


# =============================================================================
# MEMORY (simple session-based message history)
# =============================================================================

# TODO: Try mem0 for long-term semantic memory across sessions
# https://docs.mem0.ai - extracts facts, does semantic search, persists user context

# For now: simple in-memory session store (swap for Redis in production)
_sessions: dict[str, list[ModelMessage]] = {}


# =============================================================================
# REQUEST HANDLING
# =============================================================================


def get_session_key(user_id: str, session_id: str) -> str:
    return f"{user_id}:{session_id}"


async def handle_query(query: str, user_id: str, session_id: str) -> str:
    """Process a query through the main agent with conversation history."""
    session_key = get_session_key(user_id, session_id)

    # Set session context for LiteLLM tracing
    set_session_id(session_id)

    # Get existing conversation history for this session
    history = _sessions.get(session_key, [])

    # Run agent with message history (streams to console in debug mode)
    result = await run_agent(
        main_agent,
        query,
        message_history=history,
        model_settings=get_model_settings(),
    )

    # Store updated history
    _sessions[session_key] = result.all_messages()

    return result.output


# =============================================================================
# FASTAPI SERVER
# =============================================================================


class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    query: str


class ChatResponse(BaseModel):
    answer: str


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("Multi-agent system ready")
    yield
    print("Shutting down")


api = FastAPI(title="Multi-Agent Server", lifespan=lifespan)


@api.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    answer = await handle_query(request.query, request.user_id, request.session_id)
    return ChatResponse(answer=answer)


@api.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """SSE streaming endpoint - sends events as the agent works."""

    async def event_generator():
        # Create event queue for this request
        queue = create_event_queue()

        # Synthetic acknowledgment so user sees immediate feedback
        yield f"event: status\ndata: {json.dumps({'type': 'status', 'message': 'Working on it...'})}\n\n"

        # Run agent in background task
        agent_task = asyncio.create_task(
            handle_query(request.query, request.user_id, request.session_id)
        )

        try:
            while not agent_task.done():
                try:
                    # Wait for events with timeout so we can check if agent is done
                    event = await asyncio.wait_for(queue.get(), timeout=0.1)
                    yield f"event: {event['type']}\ndata: {json.dumps(event)}\n\n"
                except asyncio.TimeoutError:
                    continue

            # Drain any remaining events
            while not queue.empty():
                event = await queue.get()
                yield f"event: {event['type']}\ndata: {json.dumps(event)}\n\n"

            # Get final result and send as done event
            answer = await agent_task
            yield f"event: done\ndata: {json.dumps({'type': 'done', 'answer': answer})}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        finally:
            clear_event_queue()

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@api.get("/health")
async def health():
    return {"status": "ok"}


if __name__ == "__main__":
    uvicorn.run("server:api", host="0.0.0.0", port=8000, reload=True)
